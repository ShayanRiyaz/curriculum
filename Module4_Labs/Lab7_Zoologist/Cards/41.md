Remember that Big O notation describes the behavior of functions as input grows, in this case as the number of elements needed to sort increases.

**Step 1: Bubble Sort**

For bubble sort, there is a different time complexity for best case and worst case. This is important to note because that means that after a certain number of iterations, or elements to sort, this method becomes very inefficient. In the best case, the time it takes to sort is O(n), but this is only in the case that it sorts an already sorted list, because no sorting needs to happen. In this case, no elements need to be swapped, so major (and time consuming) work is avoided.

Once a list is only slightly unsorted, time complexity becomes O(n^2). This is because there are O(n) elements in a list, and bubble sort makes O(n) comparisons, meaning it performs O(n) comparisons on O(n) elements. So we mutiply O(n)*O(n) to get O(n^2). This is very inefficient after a few iterations of this method, since this function grows quickly. 

The number of iterations is the key here, because we use a nested for loop to implement bubble sort. So, the more sorting that needs to be done, the more times we need to enter the nested and parent for loops, increasing the number of iterations, thus increasing the time complexity.

O(n^2) is the average and worst case time complexity, so this method is mainly useful when only a couple of elements need to be sorted into their proper place.

**Step 2: Insertion Sort**

Insertion sort has the same time complexity breakdown as bubble sort. This is because, like bubble sort, no major work needs to be done when the list is sorted, in this case no elements need to be moved to the front of the list.  So best case scenario, it'll sort a sorted list, making O(n) time. 

Worst case and average case are the same, meaning it typically performs in its worst case time complexity, O(n^2). The reasoning is essentially the same as bubble sort, making insertion sort again only useful when a small amount of sorting needs to be done. 

**Step 3: Selection Sort**

Selection sort has basically the same time complexity as bubble and insertion sort, except it doesn't have an improved best case. This is because it goes through the entire list checking for the minimum value, and it won't know this until it reaches the end of the list. So, it always takes O(n^2) time, even when sorting an already sorted list, making this is the least efficient of the three methods. However, it takes O(n^2) time in every other case, just like the other two functions, so it performs the same most of the time. 

**Step 4: Space Complexity**

Bubble sort, insertion sort, and selection sort each have a space complexity of O(1). This is also referred to as constant space complexity, since it's the same no matter the amount of data they are sorting through. This is because they all sort "in place," that is, they are acting directly on the array. This means they don't have to use up extra memory storing anything in order to sort. 

Keep in mind that O(1) isn't an amount of space itself, rather, it describes the constant space taken up by the algorithms, apart from the data they're operating on. The idea is that the only space taken is by the methods themselves, and the size of the data doesn't change that. 

