Remember that Big O notation describes the behavior of functions as input grows, in this case as the number of elements needed to sort increases.

**Bubble Sort**

For bubble sort, there is a different time complexity for best case and worst case. This is important to note because that means that after a certain number of iterations, or elements to sort, this method becomes very inefficient. In the best case, the time it takes to sort is O(n), but this is only in the case that it sorts an already sorted list, because no sorting needs to happen.

Once a list is only slightly unsorted, time complexity becomes O(n^2). This is because there are O(n) elements in a list, and bubble sort makes O(n) comparisons, meaning it performs O(n) comparisons on O(n) elements. So we mutiply O(n)*O(n) to get O(n^2). This is very inefficient after a few iterations of this method, since this function grows quickly. 

O(n^2) is the average and worst case time complexity, so this method is mainly useful when only a couple of elements need to be sorted into their proper place.

**Insertion Sort**

Insertion sort has the same time complexity breakdown as bubble sort. Best case scenario, it'll sort a sorted list, making O(n) time. 

Worst case and average case are the same, meaning it typically performs in its worst case time complexity, O(n^2). The reasoning is essentially the same as bubble sort, making insertion sort again only useful when a small amount of sorting needs to be done.

**Selection Sort**

Selection sort has basically the same time complexity as bubble and insertion sort, except it doesn't have an improved best case. It always takes O(n^2) time, even when sorting an already sorted list, so this is probably the least efficient of the three methods. However, it takes O(n^2) time in every other case, just like the other two functions, so it performs the same most of the time. 

**Space Complexity**

Bubble sort, insertion sort and selection sort each have a space complexity of O(1). This is also referred to as constant space complexity, since it's the same no matter the amount of data they are sorting through. This is because they all sort "in place," that is, they are acting directly on the array. This means they don't have to use up extra memory storing anything in order to sort. 

Keep in mind that O(1) isn't an amount of space itself, rather, it describes the constant space taken up by the algorithms, apart from the data they're operating on. The idea is that the only space taken is by the methods themselves, and the size of the data doesn't change that. 

