<!--title="Cleaning Tweets"-->

## Cleaning Tweets

We want clean tweets thats consist of only English words and nothing else (such as puncutation, URLs, etc.). Only clean tweets will help us better see bigrams.

Add this to your code:
```python
tweets_no_urls = [remove_url(tweet.text) for tweet in tweets]

words_in_tweet = [tweet.lower().split() for tweet in tweets_no_urls]
```

This will further clean our text part of the tweets. 

``tweets_no_URL`` uses the function we write in the last card to remove any non English letters and/or URLs from our tweet texts list. 

``words_in_tweet`` makes sure that all the letters in the tweets are not lowercase.

Next, add:
```python
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
```
Here, we will download a list of stop words to help us filter words that we do not want to analyze in bigram models. 

Stop words are common words (such as “the”, “a”, “an”, “in”) that we don't want to have and take up vauble space.

``stop_words`` will hold our set of stop words in English so that we can use it to weed them out from our tweets.

Next, add:
```python 
tweets_nsw = [[word for word in tweet_words if not word in stop_words] for tweet_words in words_in_tweet]

# Remove collection words
collection_words = ['climatechange', 'climate', 'change']

tweets_nsw_nc = [[w for w in word if not w in collection_words] for word in tweets_nsw]
```

``tweets_nsw`` will now hold the list of tweets without anymore stop words. 
``tweets_nsw_nc`` will next hold the 