<!--title="Bigrams"-->

## Bigrams

Bigram is a n-gram where n=2.  It uses the word and its previous OR next word to calculate the probablity of the word match to a certain definition. 

For example, San Francisco is a bigram. If the computer were to read this, it would have no idea what it means.

Let's use the word "Francisco". How can the computer know this word is part of a city's name and not just a person whose name is Francisco? 

Well, it can look at the occurance () of the surrounding words. Since this is a bigram and there are only 2 words, the computer detedcts there is a previous word it can use for a clue. When it knows the word before "Francisco" is "San", the probablity of the phrase being a the city rather than a person's name is higher and therefore the computer will make a decision that this prase is a city.

## Calculating Probability with Bigram Models

For further explantion, we'll use a large sample of sentences in English, usually known as corpus.

Our exmaple corpus contains:

      1. He said thank you.
      2. He said bye as he walked through the door.
      3. He went to San Diego.
      4. San Diego has nice weather.
      5. It is raining in San Francisco.

Generally, to calculate probability, the formula is:
	(No. of times "your bigram" occurs)/ (No. of times "your word" occurs)
	
For example using this corpus, we can try:
	=(No of times “San Diego” occurs) / (No. of times “San” occurs) 
	= 2/3 
	= 0.67