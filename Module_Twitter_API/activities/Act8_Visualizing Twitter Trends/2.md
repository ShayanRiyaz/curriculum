 <!--title={Reading in the .txt file }-->

<!--badges={Web Development:}-->

# Reading in the .txt file 

Now that our data is collected, we will now prepare the data.

1. Start by importing the libraries that are needed for the analysis and visualization:

```
#Import all the needed libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import json
import seaborn as sns
import re
import collections
from wordcloud import WordCloud
```

​		pandas - "python data analysis library" takes in data and outputs Python objects with rows and 

​						columns similar to a table

​		numpy - allows for multidimensional arrays, integrating C/C++ and Fortran code

​						includes tools of linear algebra, Fourier transforms, and random number generating

​		matplotlib.pyplot - collection of commands to make matplotlib function more like MATLAB

​		json - allows for use with data stored with JSON syntax

​		seaborn - similar to matplotlib, it helps generate data visualizations

​		re - allows for use of RegEx operations in Python

​		collections - allows for data to be stored in collections such as list, set, dict and more

​		WordCloud - allows generation of word clouds in Python

2. Then, we will read the data that we have collected and process it. Remember that the output of the Streaming API is a JSON object for each tweet, with many fields that can provide very useful information. In the following block of code, we read these tweets from the .txt file where they are stored. However, lets break it down so we can see what is exactly going on here!

We setup a list to hold our tweet data and open the file where we have said tweet data.

```python
#Reading the raw data collected from the Twitter Streaming API using #Tweepy. 
tweets_data = []
tweets_data_path = 'Brexit_tweets_1.txt'
tweets_file = open(tweets_data_path, "r") ##change to filename
```

Next, we go through our file and add all the tweet data to our list. What the `try` block does is simple: its test if we can do an action (in this case open and read a line in a file). If we cannot we do the code in the `except` block(this case go to the next line in the file).

```python
for line in tweets_file:
    try:
        tweet = json.loads(line)
        tweets_data.append(tweet)
    except:
        continue
```

In the piece of code, you will have to change the string ***tweets_data_path\*** to the name of the document where you have stored your data. 

```tweets_data``` will be holding data newly formatted to JSON. 

We will use the ```open()``` function to open the file. For each line of the open file, we will try to turn each line of data into a JSON format and then add it to the like of tweets_data. 

If the line can not be turned into JSON format, the except will catch the error to prevent the program from crashing. The error will be ignored and the program will continue to the next line of the file.

3. While downloading the data, there might be connection issues or other errors. To prevent crashing and slow downs, add this line of code:

```python
#Error codes from the Twitter API can be inside the .txt document, #take them off
tweets_data = [x for x in tweets_data if not isinstance(x, int)]
```

4. Now, you can see how many *tweets* we have collected by inserting this line of code into your main function:

```python
print("The total number of Tweets is:",len(tweets_data))
```