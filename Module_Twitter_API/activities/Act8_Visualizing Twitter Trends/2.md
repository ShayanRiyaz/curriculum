 <!--title={Reading in the .txt file }-->

<!--badges={Web Development:}-->

# Reading in the .txt file 

Now that our data is collected, we will now prepare the data.

1. Start by importing the libraries that are needed for the analysis and visualization:

```
#Import all the needed libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import json
import seaborn as sns
import re
import collections
from wordcloud import WordCloud
```

​		pandas - "python data analysis library" takes in data and outputs Python objects with rows and 

​						columns similar to a table

​		numpy - allows for multidimensional arrays, integrating C/C++ and Fortran code

​						includes tools of linear algebra, Fourier transforms, and random number generating

​		matplotlib.pyplot - collection of commands to make matplotlib function more like MATLAB

​		json - allows for use with data stored with JSON syntax

​		seaborn - similar to matplotlib, it helps generate data visualizations

​		re - allows for use of RegEx operations in Python

​		collections - allows for data to be stored in collections such as list, set, dict and more

​		WordCloud - allows generation of word clouds in Python

2. Then, we will read the data that we have collected and process it. Remember that the output of the Streaming API is a JSON object for each tweet, with many fields that can provide very useful information. In the following block of code, we read these tweets from the .txt file where they are stored. However, lets break it down so we can see what is exactly going on here!

We setup a list to hold our tweet data and open the file where we have said tweet data.

```python
#Reading the raw data collected from the Twitter Streaming API using #Tweepy. 
tweets_data = []
tweets_data_path = 'Brexit_tweets_1.txt'
tweets_file = open(tweets_data_path, "r") ##change to filename
```

Next, we go through our file and add all the tweet data to our list. What the `try` block does is simple: its test if we can do an action (in this case open and read a line in a file). If we cannot we do the code in the `except` block(this case go to the next line in the file).

```python
for line in tweets_file:
    try:
        tweet = json.loads(line)
        tweets_data.append(tweet)
    except:
        continue
```

In the piece of code, you will have to change the string ***tweets_data_path\*** to the name of the document where you have stored your data. 

3. While downloading the data, there might be connection issues or other errors. To fix this, go into the .txt file on where we have stored the tweets and delete the error codes. To get rid of them, run the following line of code after reading the .txt file.

```python
#Error codes from the Twitter API can be inside the .txt document, #take them off
tweets_data = [x for x in tweets_data if not isinstance(x, int)]
```

4. Now, you can see how many *tweets* we have collected by inserting this line of code into your main function:

```python
print("The total number of Tweets is:",len(tweets_data))
```