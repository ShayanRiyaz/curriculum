<!--title={Merge Sort Time Complexity}-->

<!--badges={Algorithms:10,Python:5}-->

<!--concepts={Merge sort}-->

Merge sort is similar to insertion sort in that all three cases have the same time complexity. Again, here is the code for the algorithm as a reference:

```Python
def mergeSort(alist):
    if len(alist) > 1:
        mid = len(alist) // 2 # integer division
        lefthalf = alist[:mid]
        righthalf = alist[mid:]
        mergeSort(lefthalf)
        mergeSort(righthalf)

        i = 0
        j = 0
        k = 0
        while i < len(lefthalf) and j < len(righthalf):
        	if lefthalf[i] <= righthalf[j]:
        		alist[k]=lefthalf[i]
        		i=i+1
        	else:
        		alist[k]=righthalf[j]
        		j=j+1
        	k=k+1

        while i < len(lefthalf):
        	alist[k]=lefthalf[i]
        	i=i+1
        	k=k+1

        while j < len(righthalf):
        	alist[k]=righthalf[j]
        	j=j+1
        	k=k+1


```

Merge sort's time complexity for all three cases is O(nlogn). 

###Where does Log n come from?

The logn in the time complexity comes from the recursion happening two lines below the first if statement:

```python
def mergeSort(alist):
    if len(alist) > 1:
        mid = len(alist) // 2 # integer division
        lefthalf = alist[:mid]
        righthalf = alist[mid:]
        mergeSort(lefthalf)
        mergeSort(righthalf)
```

As you can see from the above code, whenever the algorithm recurses, only half of the list is passed on, meaning it is doing only half the work. Since it halves the list every recursion regardless of the list's state (ordered or not), the time complexity for this part of the code is O(logn) for all cases.

###Where does the n come from?

The n comes from the three while-loops present in the code: 

```python
while i < len(lefthalf) and j < len(righthalf):
        	if lefthalf[i] <= righthalf[j]:
        		alist[k]=lefthalf[i]
        		i=i+1
        	else:
        		alist[k]=righthalf[j]
        		j=j+1
        	k=k+1

        while i < len(lefthalf):
        	alist[k]=lefthalf[i]
        	i=i+1
        	k=k+1

        while j < len(righthalf):
        	alist[k]=righthalf[j]
        	j=j+1
        	k=k+1

```

The n comes from while-loops iterating over half of the list n times.  Due to the conditions given to the while-loops, every time the algorithm recurses, only one while-loop will trigger. This prevents the time complexity from becoming O(n^2) or O(n^3). Therefore, when you combine the time complexity from the recursion and from while-loops, you get O(nlogn).