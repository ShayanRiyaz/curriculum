**Time complexity** allows us to describe **how the time taken to run a function grows as the size of the input of the function grows.**

Take this function as an example:

```python
value = [1, 2, 3, 4, 5]

def valueSum(value):
    sum = 0
    for i in range(value):
        sum = sum + i
    return sum
```
The amount of time this function takes to run will grow as the number of elements in the array increase. If the elements in the array `value` went up to 1 million, the amount of time it would take for the function to compute would be much higher than if the array only went up to 10. 

Time complexity answers the question: "At what rate does the time increase for a function as the input increases?". **However**, it does not answer the question, "How long does it take for a function to compute?"  because the answer relies on factors such as hardware and language.

The rate of a function's growth can be described as **constant**, **linear**, **quadratic**, and so on.

[//]: # "insert 'timecomplexity' image"


<img src="https://projectbit.s3-us-west-1.amazonaws.com/darlene/labs/TimeSpace1.jpeg">


In our function, `valueSum()`, this is what the function's growth would look like. 

**n** is the number of elements within the array values. As `n` grows, so does the execution time in a linear fashion. 

Let's look at this function a little more closely. 



