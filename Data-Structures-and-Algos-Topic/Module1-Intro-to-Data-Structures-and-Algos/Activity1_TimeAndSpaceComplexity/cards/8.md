Rewriting our `valueSum` function but in **Big-O notation**, we would have:

```
Time(Input) = O(1) + O(n) + O(1). 
```

We've written the lines of the function in **Big-O Notation**, but now we need to write the function itself in **Big-O Notation**. The way we do that is to choose the term that is growing the fastest in the function (the dominant term) as *n* gets very large. We then disregard any coefficients of that term. Then assign that term for the function. 

For `valueSum`, the fastest growing term is **c2 * n** . If we ignore **c2** , we are left with just `n`. We now know that the time complexity of `valueSum` is **O(n)** and the runtime of `valueSum` grows in a linear fashion.

Let's look at another example.

In the function `listInList`:

```python
keypad = [[1, 2, 3], 
          [4, 5, 6],
          [7, 8, 9],
          [0]] 
def listInList(keypad):
    sum = 0
    for row in keypad:
        for i in row:
            sum += i
    return sum
```

`listInList` expressed as a function of time looks like this:

$$
Time(Input) = c1 + c2 * n^2 + c3
$$
We know that **c1** corresponds to **O(1)** . 
**c2 * n^2** is written as **O(n^2)**. 
**c3** is written as **O(1)** since it operates under constant time.

The fastest growing term is **c2 * n^2**  so we know that this term defines how our function is written in **Big-O Notation**. Dropping the coefficient, **c2**, our function is expressed as **O(n^2)**. in **Big-O Notation**. This tells us that `listInList` grows in quadratic time. 

As for our last example, let's make a quick edit to our function `listInList` and call it `listInList2`. 

```python
keypad = [[1, 2, 3], 
          [4, 5, 6],
          [7, 8, 9],
          [0]]
def listInList2(keypad):
    sum = 0
    for row in keypad:
        for i in row:
            sum += i
    for row in keypad:
        for i in row:
            sum += i
    while sum <= 100:
        sum += 1
    return sum
```

Notice that we doubled the amount of *for loops* in our function. We have to account for that when writing `listInList2` as a function of time. Since we have extra lines in our code, we need to add a term to our **Time(Input)** expression. To account for a double for loop, we can simply add another **n^2 * c2** to our **Time(Input) expression**. 

$$
T(I) = c1 + (c2 * n^2) + (c2 * n^2) + (c3 * n) + c4
$$


The fastest growing term is **n^2 * c2** but we have two of them. Since **n^2 * c2** corresponds to **O(n^2)**, we are left with:

**Time(Input) = O(n^2) + O(n^2)**

This can simply be rewritten as:

**Time(Input) = 2 * O(n^2)**

It might be intuitive to think that our function written in **Big-O notation** would be **2 * O(n^2)** or even **O(2n^2)**, but we have to remember that **Big-O notation only tells us what time a function grows in**. **O(n^2)** and  **O(2n^2)** both grow in the same fashion, quadratic, so we can simply write the function as **O(n^2)**. 

It is very important to remember that Big-O notation is determined by the fastest growing term as `n` gets very large. When n is small, c3 * n will grow faster than c2 * n^2. **Big-O notation is determined when n is very large** because time and space complexities focus on when inputs are very large; not small. 



### Time expressed in Big-O Notation

| Time             | Big O Notation |
| ---------------- | -------------- |
| Constant Time    | O(1)           |
| Linear Time      | O(n)           |
| Quadratic Time   | O(n^2)         |
| Logarithmic Time | O(log n)       |
| Factorial Time   | O(n!)          |

In the table, there are some other times you should know of. The graph lists the growth of functions from slowest to fastest. A function in **O(1)**'s runtime grows the slowest, and a function in **O(n!)** grows the fastest. Needless to say, we want our functions to be **constant or linear time.** 

